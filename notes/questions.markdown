---
layout: default
title: questions to answer
type: notes
---

* Bayesian nonparametrics keeps building distributions over 'richer' datastructures (e.g. other distributions, binary feature matrices, partitions, kd-tree structures, dags ...). Is there a methodology to say 'I want a nonparametric distribution over structures like X' and work from there?

* David MacKay said we should be looking for '1-and-a-half-shot' learning algorithms -- 1-shot may be unrealistic, but all our current methods are very iterative. Problems aren't really adversarial -- the worst case isn't the one we're usually in. How can we do inference that works efficiently in *most* cases...

* There are various combinations of MCMC and SMC now -- are there properties we can extract to construct a 'taxonomy' which might have unimplemented/unexplored spaces in it?

* How can abstract algebra benefit machine learning, bayesian or otherwise? In cases where sampling regimes produce 'incomparable' hypotheses (as in the standard gibbs sampler for topic models) can we render them into the same 'basis' in a way that allows them to be compared? Can algebra reduce the 'dimensionality' of some problems (by subtracting out 'equivalent' objects?) 

* How can machine learning deal with recursive/self-referential processes. How can machine learning deal with data generated by processes which seek to optimize some utility?  Can we combine machine learning with game theory -- e.g. given some behavior which results from interactions among many agents, can we learn about those agents utility functions?

* It seems like the world does all kinds of things which aren't readily computable -- it solves the n-body problem and such. Can we find some way to express this separation -- can we take advantage of particular physical systems to answer particular kinds of questions quickly?

