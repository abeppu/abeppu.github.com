<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" type="text/css" href="/css/linotype.css" />
    <script type="text/javascript" src="/js/d3.js"></script>
    <script type="text/javascript" src="/js/jquery-1.6.2.min.js"></script>
    <title>Monoids, monads, and Bayesian inference - Aaron's Digital Wastebook</title>
  </head>

  <body>
    <script type="text/javascript" src="http://mathjax.connectmv.com/MathJax.js"></script>
    <link rel="shortcut icon" href="/images/4color.png" />
    <div id="doc" class="yui-t6"> 
      <div id="hd"> 
	<ul class="path"> 
	  <li><a href="/index.html">Home</a></li> 
	  
	    <li><a href="/index.html">Blog</a></li>
	  
	  
	  <!--     <li><a href="/{}"></a></li>  -->
	  <li class="current">Monoids, monads, and Bayesian inference</li> 
	</ul> 	
	<h1>Aaron's Digital <br> Wastebook</h1> 
	<!--<h2>in Aaron's Digital Wastebook</h2> -->
      </div> 
      <div id="bd">
        <div id="yui-main"> 
          <div id="body" class="yui-b"> 	   
	    <div id="post">
  <h1 class="title"><a href="/blog/drafts/2010-12-03-monoids-monads-and-bayesian-inference.html">
    Monoids, monads, and Bayesian inference
  </a></h2>
  <div id="date">published Liquid error: undefined method `strftime' for nil:NilClass</div>
  <p>For a while now, I’ve been trying to learn a bit about abstract algebra and the functional languages which use algebraic structures as patterns, in the hopes of understanding how this stuff could be useful to machine learning. I don’t really yet follow a lot of what goes on with so-called <a href="http://en.wikipedia.org/wiki/Algebraic_statistics">“algebraic statistics”</a> and <a href="http://en.wikipedia.org/wiki/Information_geometry">“information geometry”</a>, but I haven’t yet found what I’m looking for. </p>

<p>But today I realized that the most basic, high level way that we can cast some machine learning areas into algebraic terms hadn’t shown up in my reading, so I’ve decided to flesh it out here.</p>

<p>I think this is the kind of obvious thing where anyone familiar with the rudiments of both Bayesian stats and the relevant algebra/FP structures would view this as obvious, but it took me a while to realize, so I figured I may as well record it and flesh out the details.</p>

<h2 id="towards-a-belief-monoid">Towards a belief monoid</h2>

<p>A few refresher points :</p>

<ul>
  <li>From an algebraic perspective, a <a href="http://en.wikipedia.org/wiki/Monoid#Definition">monoid</a> is like a group without inverses; it’s a set combined with an associative binary operation, where the set is closed under the operation, and there’s an identity element.</li>
  <li>Analogously, in languages like Haskell and Scala, <a href="">monoids</a> are foldable structures. The identity element is often a default or empty value, and the binary operation is how pieces are folded into an aggregating value. An example is a the set of lists, which is closed under concatenation (which is associative), with the identity element being the empty list.</li>
  <li>In Bayesian inference, we start out with a prior distribution $p(H)$ which represents beliefs in the absence of any evidence. Given some observed set of data $d<em>1$ we can form the posterior distribution $p(H\vert d</em>1) \propto p(H)p(d_1\vert H)$.</li>
  <li>We can use $p(H\vert d<em>1)$ as a prior when we receive a new dataset $d</em>2$, to get a new posterior $p(H\vert d<em>1, d</em>2) \propto p(H)p(d<em>1\vert H)p(d</em>2\vert H)$.</li>
  <li>In general we can chain these together eventually arriving at $p(H\vert d<em>1 \ldots d_k) \propto p(H) \prod</em>{i=1}^k p(d_i\vert H)$</li>
</ul>

<p>Synthesizing the above points, the conclusion is that we can form a monoid from the likelihood components of our eventual posterior distribution. At any point, we can weight this combined likelihood distribution according to a prior, and get a posterior. Unfortunately, we can’t easily combine two posteriors informed by different data, because we would be weighting by the prior twice : $p(H \vert d<em>1)p(H\vert d</em>2) \propto p(H)p(H)p(H\vert d<em>1)p(H\vert d</em>2)$</p>

<p>TODO : Develop this as code</p>

<h2 id="conjugate-priors">Conjugate priors</h2>

<p>The monoid structure defined above is true of distributions in their abstract sense, but a distribution isn’t something we can instantiate. It’s not obvious what the data structure is for an arbitrary distribution. So what does this look for some concrete distributions we might actually care about?</p>

<p>Trivially, if our hypothesis space is not only discrete, but has some small cardinality, then we can represent our distributions easily, as lists of probabilities when there’s some obvious enumeration of the hypotheses, or as a map when no such obvious ordering exists. Basically, this is a histogram, and as we receive evidence, we just update the weights of the bars (and rescale so they sum to 1).</p>

<p>But frequently, we want our beliefs to have some manageable form, no matter what data we see.</p>

<pre><code>class Distribution d h where
    probability :: d -&gt; h -&gt; RealFloat

data Binomial = Binomial Float Float

instance Distribution Binomial where
    prob (Binomial a b) x = x**a * (1 - x)**b
</code></pre>

<p>TODO : replace leftarrows with ‘drawn from’ symbols</p>

<p>For example, perhaps the most basic case looks like this :</p>

<p>$Z \sim \text{Beta}(\alpha, \beta)$<br />
$X_i \sim \text{Bernoulli}(z)$ with $0 \leq i \leq k$</p>

<p>Here $\alpha$ and $\beta$ are hyperparameters, $z$ is the latent variable we care about, and our observed $k$ datapoints are just binary variables. The cool thing is that </p>

<script type="math/tex; mode=display">P(Z = z) \propto z^{\alpha} * (1-z)^{\beta}
P(Z = z | X_i = \text{True}) \propto z^{\alpha + 1} * (1 - z)^{\beta}  
P(Z = z | X_i = \text{False}) \propto z^{\alpha} * (1 - z)^{\beta + 1}</script>

<p>How can we encapsulate these relationships in code?</p>

<p>Distribution </p>

<h2 id="update-operations-are-monadic">Update operations are monadic</h2>

<p>instance Monad Distribution where
    return x =     </p>

<p>class Likelihood f where
    likelihood :: h -&gt; d -&gt; RealFloat</p>


</div>
 

          </div>
	</div>
	<div id="sidebar" class="yui-b">
	  <table><tr><td> 
		<ul> 
		  <li><a href="/index.html">Home</a></li> 
		  <li><a href="/blog/about.html">About</a></li> 
		  <li><a href="/blog/resume.html">Resume</a></li> 
		</ul> 
	  </td></tr></table> 
	</div>
      </div>      

    </div>
    <dstatic.flickr.com/5050/535iv>
      <!--  give credit where credit is due for fonts  -->
      <br><br>
      Font Credits:
      <font face="Linux Libertine Regular"><a href="http://www.linuxlibertine.org/index.php?id=2&L=1">linux libertine</a></font>&nbsp&nbsp
      <font face="Biolinum Regular"><a href="http://www.linuxlibertine.org/index.php?id=2&L=1">linux biolinum</a></font>&nbsp&nbsp
      <font face="skyhook"><a href="http://www.fontomtype.de/pages/2010/10/23/skyhookmono/">skyhook mono</a></font>
    </div>
  </body>
</html>
